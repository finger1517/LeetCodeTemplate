1.
n-gram特点：n-gram是一种简单的语言模型，核心思想是看第一个词确定后，后面次在前面词出现条件下出现的概率。词语概率的联乘就是这句话出现的概率，这句话概率的大小也体现了这句话是否自然。n-gram利用了马尔科夫链的思想。但是ngram对计算资源的消耗很大，
n-gram适用场景：我们有时候需要判断一句话出现的概率是多少，或者判断讲话的人是否经常说这样的话，可以用n-gram模型。ngram还有别场景，如模糊匹配，也就是判断句子s和t是否相似，给出前面的词，我们可以计算和前面词语最匹配的词语当做潜在项提供给用户，可以用于搜索引擎中。也可以自动写作诗词文章。

word2vec特点：是目前常见的词嵌入模型之一，有两种网络结构cbow\skip-gram，一般使用skipgram，如果两个词的词向量很相近，那么他们可能在相同的上下文中出现。词向量体现的是低层级的信息，而LDA中的主题模型更倾向于文章主题这一层信息。
word2vec适用场景：有对单词向量画的工作都会用到word2vec。word2vec已经成为了词向量嵌入的一个标准，该思想也影响了很多推荐系统，广告系统的设计如item2vec。在推荐时规避相似的item.

LDA特点：叫做隐狄利克雷模型，文档中单词的共同出现的关系来对单词进行按照主题的聚类，是一种基于概率图的生成式模型。
LDA适合场景：做文章主题的分类，也可以作为word embedding的输入，可以增强文章的分类效果。也可以计算文章的相似度，从而做一些情绪分析。

2. RNN类模型相对attention模型有什么优缺点：
- RNN类的模型结构能够很好的利用序列之间的关系，可以用于NLP以及语音领域如文本分类，情感分析，意图识别，机器翻译等。相比attention，rnn参数比较多，但是可以学习序列的顺序关系，擅长时间序列的相关的模型。
- attention 是一种应用于神经网络的计算规则，一般用于seq2seq的结构，它的作用是：能够根据模型目标有效的聚焦编码器的输出结果，当它作为解码器的输入时提升效果，改善了RNN解码器输出是单一定长的张量，无法存储过多信息的情况。增强了模型的表征能力。同时attention模块每一步的结果不依赖于上一步，可以做成并行的模式，加速计算。缺点就是不适合时序的模型预测。
